defmodule Mix.Tasks.LlmContextCompress do
  use Mix.Task

  @shortdoc "Compresses LLM context documentation for efficient LLM consumption"

  @moduledoc """
  This Mix task takes a full LLM context markdown file (like those generated by `llm_ingest`)
  and compresses it by extracting only the most relevant sections for AI analysis,
  reducing token count without losing critical context.

  It focuses on:
  - Project general information and structure.
  - Explicit AI analysis notes (like the 'Notes for AI Analysis' section).
  - Key code files identified as part of a specific feature (e.g., `lib/mix/tasks/**`).
  - Summarized documentation like `docs/add-llm-provider.txt`.

  USAGE:
    mix llm_context_compress --input my_large_context.md --output compressed_context.md
    mix llm_context_compress -i doc/llm-ingest-mix_task.md -o doc/llm-ingest-mix_task_compressed.md
    mix llm_context_compress --help
  """

  @separator "\n---\n" # The separator used in the input markdown file

  @impl true
  def run(args) do
    {opts, _remaining_args, _} =
      OptionParser.parse(args,
        switches: [
          input: :string,
          output: :string,
          help: :boolean
        ],
        aliases: [i: :input, o: :output]
      )

    if opts[:help] do
      show_help()
      System.halt(0)
    end

    input_file = opts[:input]
    output_file = opts[:output] || "doc/llm-ingest-compressed.md" # Default output path

    unless input_file do
      Mix.shell().error("Error: --input (or -i) is required. Specify the path to the input markdown file.")
      show_help()
      System.halt(1)
    end

    unless File.exists?(input_file) do
      Mix.shell().error("Error: Input file '#{input_file}' not found.")
      System.halt(1)
    end

    Mix.shell().info("Compressing context from '#{input_file}' to '#{output_file}'...")

    case File.read(input_file) do
      {:ok, full_content} ->
        # Ensure the output directory exists
        File.mkdir_p!(Path.dirname(output_file))

        compressed_content = compress_content(full_content)

        case File.write(output_file, compressed_content) do
          :ok ->
            Mix.shell().info("âœ… Successfully compressed context to '#{output_file}'")
            Mix.shell().info("Original size: #{byte_size(full_content)} bytes")
            Mix.shell().info("Compressed size: #{byte_size(compressed_content)} bytes")
            Mix.shell().info("Compression ratio: #{(byte_size(compressed_content) / byte_size(full_content) * 100) |> Float.round(2)}%")
          {:error, reason} ->
            Mix.shell().error("Error writing to output file '#{output_file}': #{reason}")
            System.halt(1)
        end

      {:error, reason} ->
        Mix.shell().error("Error reading input file '#{input_file}': #{reason}")
        System.halt(1)
    end
  end

  defp show_help do
    Mix.shell().info("""
    #{@shortdoc}

    USAGE:
        mix llm_context_compress --input INPUT_FILE [OPTIONS]

    OPTIONS:
        --input, -i INPUT_FILE   Path to the input LLM context markdown file (e.g., generated by llm_ingest).
        --output, -o OUTPUT_FILE Path to save the compressed output markdown file (default: doc/llm-ingest-compressed.md).
        --help                   Show this help message.

    EXAMPLES:
        # Compress a specific context file
        mix llm_context_compress -i doc/llm-ingest-mix_task.md -o doc/llm-ingest-mix_task_compressed.md

        # Use default output file
        mix llm_context_compress --input doc/my_feature_context.md

    This task aims to reduce the token count for LLMs by intelligently extracting
    and summarizing the most relevant parts of a detailed context document.
    """)
  end

  # No @doc for private functions
  defp compress_content(full_content) do
    sections = String.split(full_content, @separator, trim: true)

    # 1. Extract Project Name and Description
    project_info = Enum.find(sections, fn section ->
      String.starts_with?(section, "# ") and not String.contains?(section, "Feature:")
    end)

    # 2. Extract AI Analysis Notes
    ai_notes = Enum.filter(sections, fn section ->
      String.starts_with?(section, "## Notes for AI Analysis") or
      String.starts_with?(section, "**Mix_task Feature Context:**") or
      String.starts_with?(section, "**General Elixir Project Reading Guide:**")
    end)

    # 3. Extract Project Structure
    project_structure = Enum.find(sections, fn section ->
      String.starts_with?(section, "## Project Structure")
    end)

    # 4. Extract specific relevant file contents
    # Based on "Include patterns: `lib/mix/tasks/**`, `docs/add-llm-provider.txt`"
    relevant_files_content = Enum.filter(sections, fn section ->
      String.starts_with?(section, "### ") &&
      (String.contains?(section, "lib/mix/tasks/") || String.contains?(section, "docs/add-llm-provider.txt"))
    end)
    # Refine content for specific files to be more concise if needed
    |> Enum.map(&refine_file_content/1)

    # Assemble the compressed content
    compressed_parts = [project_info] ++ ai_notes ++ [project_structure] ++ relevant_files_content

    # Join them back with the separator for LLM readability
    Enum.reject(compressed_parts, &is_nil/1) |> Enum.join(@separator)
  end

  # No @doc for private functions
  defp refine_file_content(file_section) do
    # Extract the file path
    [header_line | body_lines] = String.split(file_section, "\n", parts: 2)
    file_path = String.replace(header_line, "### ", "") |> String.trim()
    content = List.first(body_lines) # Get the rest of the content after header

    cond do
      # For 'docs/add-llm-provider.txt', summarize the steps rather than raw code.
      # The LLM doesn't need to 'execute' this, but understand the process.
      String.contains?(file_path, "docs/add-llm-provider.txt") ->
        summarize_add_llm_provider_doc(file_path, content)

      # For 'llm-workflow.ex', focus on `run` function, `run_feature_workflow`,
      # `analyze_with_ai`, `create_github_ticket` and AI-related functions.
      # Omit verbose debug/shell info within functions, but keep function signatures.
      String.contains?(file_path, "lib/mix/tasks/llm-workflow.ex") ->
        summarize_llm_workflow_code(file_path, content)

      # For other mix tasks, similarly summarize key functions.
      String.contains?(file_path, "lib/mix/tasks/") ->
        summarize_mix_task_code(file_path, content)

      true ->
        # For other files, return as is or apply a generic trimming
        file_section
    end
  end

  # No @doc for private functions
  defp summarize_add_llm_provider_doc(file_path, content) do
    summarized_steps =
      content
      |> String.split("## Step ", trim: true)
      |> Enum.map(fn step_block ->
        [header | _body] = String.split(step_block, "\n", parts: 2)
        step_number_title = String.trim(header)
        # Extract the "ACTION" line if present
        action_line = Regex.scan(~r/ACTION: (.+)/, step_block)
                      |> List.first()
                      |> case do
                        [_, action_text] -> "- Action: #{action_text}"
                        _ -> ""
                      end
        "Step #{step_number_title}\n#{action_line}" |> String.trim()
      end)
      |> Enum.join("\n\n")

    """
    ### #{file_path}

    This document outlines the step-by-step process for adding a new LLM provider.
    The key steps involve:

    #{summarized_steps}

    It also includes considerations for API formats, model naming, error handling,
    and testing the new provider.
    """
  end

  # No @doc for private functions
  defp summarize_llm_workflow_code(file_path, content) do
    # Initial list is assigned to `initial_extracted_info` to avoid the warning.
    initial_extracted_info = [
      "## Function Signatures and Key Logic for #{file_path}\n",
      "```elixir"
    ]

    # Use a pipeline or explicit re-assignment to build up the list
    extracted_info = initial_extracted_info
    |> (fn acc ->
      moduledoc_match = Regex.scan(~r/@moduledoc """\n(.*?)"""/s, content)
      if Enum.any?(moduledoc_match) do
        acc ++ ["\n" <> (Enum.at(Enum.at(moduledoc_match, 0), 1) |> String.trim()) <> "\n"]
      else
        acc
      end
    end).()
    |> (fn acc ->
      shortdoc_match = Regex.scan(~r/@shortdoc "(.*?)"/s, content)
      if Enum.any?(shortdoc_match) do
        acc ++ ["\n" <> (Enum.at(Enum.at(shortdoc_match, 0), 1) |> String.trim()) <> "\n"]
      else
        acc
      end
    end).()
    |> (fn acc ->
      option_parser_match = Regex.scan(~r/OptionParser\.parse\(args,.*?switches: \[(.+?)\]\s*\)/s, content)
      if Enum.any?(option_parser_match) do
        switches_content = Enum.at(Enum.at(option_parser_match, 0), 1)
        acc ++ ["\nOptionParser Switches:\n```elixir\n#{switches_content}\n```\n"]
      else
        acc
      end
    end).()

    functions_to_extract = [
      "run", "run_feature_workflow", "run_full_analysis_workflow", "analyze_with_ai",
      "create_github_ticket", "call_ai_analysis", "make_real_ai_request",
      "make_openai_request", "make_claude_request", "make_ollama_request",
      "make_vllm_request", "make_groq_request", "make_perplexity_request",
      "make_cohere_request", "make_openrouter_request", "get_ai_client",
      "get_openai_client", "get_claude_client", "get_ollama_client",
      "get_vllm_client", "get_groq_client", "get_perplexity_client",
      "get_cohere_client", "get_openrouter_client", "parse_ai_analysis_response",
      "extract_json_from_response", "ensure_finch_started"
    ]

    extracted_functions = Enum.map(functions_to_extract, fn func_name ->
      function_regex = ~r/(defp?\s+#{func_name}\s*\(.*?\)\s*do.*?end)/s
      case Regex.scan(function_regex, content) do
        [[_full_match, func_body]] ->
          brief_body = func_body
                       |> String.split("\n")
                       |> Enum.filter(fn line ->
                         String.starts_with?(String.trim(line), "def") or
                         String.starts_with?(String.trim(line), "case ") or
                         String.contains?(String.trim(line), "Mix.shell().error") or
                         String.contains?(String.trim(line), "Mix.shell().info")
                       end)
                       |> Enum.take(3)
                       |> Enum.join("\n")

          "\n" <> brief_body <> "\n  # ... (further logic omitted for brevity)\nend\n"
        _ ->
          nil
      end
    end)
    |> Enum.reject(&is_nil/1)

    # Compile all parts
    extracted_info ++ extracted_functions ++ ["```"] |> Enum.join("\n")
  end

  # No @doc for private functions
  defp summarize_mix_task_code(file_path, content) do
    # Initial list is assigned to `initial_extracted_info` to avoid the warning.
    initial_extracted_info = [
      "## Function Signatures and Key Logic for #{file_path}\n",
      "```elixir"
    ]

    # Use a pipeline or explicit re-assignment to build up the list
    extracted_info = initial_extracted_info
    |> (fn acc ->
      moduledoc_match = Regex.scan(~r/@moduledoc """\n(.*?)"""/s, content)
      if Enum.any?(moduledoc_match) do
        acc ++ ["\n" <> (Enum.at(Enum.at(moduledoc_match, 0), 1) |> String.trim()) <> "\n"]
      else
        acc
      end
    end).()
    |> (fn acc ->
      shortdoc_match = Regex.scan(~r/@shortdoc "(.*?)"/s, content)
      if Enum.any?(shortdoc_match) do
        acc ++ ["\n" <> (Enum.at(Enum.at(shortdoc_match, 0), 1) |> String.trim()) <> "\n"]
      else
        acc
      end
    end).()
    |> (fn acc ->
      option_parser_match = Regex.scan(~r/OptionParser\.parse\(args,.*?switches: \[(.+?)\]\s*\)/s, content)
      if Enum.any?(option_parser_match) do
        switches_content = Enum.at(Enum.at(option_parser_match, 0), 1)
        acc ++ ["\nOptionParser Switches:\n```elixir\n#{switches_content}\n```\n"]
      else
        acc
      end
    end).()

    common_functions_to_extract = [
      "run", "show_help", "generate_feature_context", "analyze_feature_with_ai",
      "call_openai_api", "dependencies_to_patterns", "trace_static_dependencies",
      "trace_runtime_dependencies", "is_app_module?", "module_to_file_pattern",
      "find_source_by_convention", "get_enhanced_feature_config", "write_files_content",
      "generate_folder_structure"
    ]

    extracted_functions = Enum.map(common_functions_to_extract, fn func_name ->
      function_regex = ~r/(defp?\s+#{func_name}\s*\(.*?\)\s*do.*?end)/s
      case Regex.scan(function_regex, content) do
        [[_full_match, func_body]] ->
          brief_body = func_body
                       |> String.split("\n")
                       |> Enum.filter(fn line ->
                         String.starts_with?(String.trim(line), "def") or
                         String.starts_with?(String.trim(line), "case ") or
                         String.contains?(String.trim(line), "Mix.shell().error") or
                         String.contains?(String.trim(line), "Mix.shell().info")
                       end)
                       |> Enum.take(3)
                       |> Enum.join("\n")
          "\n" <> brief_body <> "\n  # ... (further logic omitted for brevity)\nend\n"
        _ ->
          nil
      end
    end)
    |> Enum.reject(&is_nil/1)

    # Compile all parts
    extracted_info ++ extracted_functions ++ ["```"] |> Enum.join("\n")
  end
end
